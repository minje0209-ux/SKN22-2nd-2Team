"""
KKBox Churn Prediction - Data Preprocessing & Feature Engineering
ì‘ì„±ì: ì´ë„í›ˆ (LDH)
ì‘ì„±ì¼: 2025-12-16

ì´ ëª¨ë“ˆì€ KKBox ë°ì´í„°ì…‹ì˜ ì „ì²˜ë¦¬ ë° í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
EPIC 1ì—ì„œ ì •ì˜í•œ ì›ì¹™ì„ ì¤€ìˆ˜í•©ë‹ˆë‹¤:
- ì˜ˆì¸¡ ì‹œì  (T): 2017-04-01
- ê´€ì¸¡ ìœˆë„ìš°: 2017-03-01 ~ 2017-03-31 (30ì¼)
- ë¯¸ë˜ ì •ë³´ ëˆ„ìˆ˜ ê¸ˆì§€
"""

import pandas as pd
import numpy as np
from pathlib import Path
from typing import Tuple, Optional, Dict, Any
from sklearn.model_selection import train_test_split
import warnings

warnings.filterwarnings('ignore')

# ============================================
# ì„¤ì •
# ============================================
PREDICTION_TIME = pd.Timestamp('2017-04-01')  # ì˜ˆì¸¡ ì‹œì  T
OBSERVATION_START = pd.Timestamp('2017-03-01')
OBSERVATION_END = pd.Timestamp('2017-03-31')

# ë°ì´í„° ê²½ë¡œ
DATA_DIR = Path(__file__).parent.parent / 'data'

# ê²°ì¸¡ì¹˜/ì´ìƒì¹˜ ì²˜ë¦¬ ê·œì¹™
PREPROCESSING_RULES = {
    'age': {
        'type': 'ì´ìƒì¹˜ ì²˜ë¦¬',
        'rule': '0 < age < 100 ë²”ìœ„ ì™¸ â†’ ì¤‘ì•™ê°’ ëŒ€ì²´',
        'reason': 'ë¹„í˜„ì‹¤ì ì¸ ë‚˜ì´ê°’ (ìŒìˆ˜, 0, 100ì„¸ ì´ìƒ) ì œê±°'
    },
    'gender': {
        'type': 'ê²°ì¸¡ì¹˜ ì²˜ë¦¬',
        'rule': 'NaN â†’ "unknown"',
        'reason': 'ì„±ë³„ ë¯¸ì…ë ¥ ì‚¬ìš©ì ë³„ë„ ë²”ì£¼ë¡œ ì²˜ë¦¬'
    },
    'city': {
        'type': 'ê²°ì¸¡ì¹˜ ì²˜ë¦¬',
        'rule': 'NaN â†’ 0',
        'reason': 'ë„ì‹œ ë¯¸ì…ë ¥ì„ 0ìœ¼ë¡œ ì²˜ë¦¬'
    },
    'registered_via': {
        'type': 'ê²°ì¸¡ì¹˜ ì²˜ë¦¬',
        'rule': 'NaN â†’ 0',
        'reason': 'ê°€ì… ê²½ë¡œ ë¯¸ì…ë ¥ì„ 0ìœ¼ë¡œ ì²˜ë¦¬'
    },
    'numeric_features': {
        'type': 'ê²°ì¸¡ì¹˜ ì²˜ë¦¬',
        'rule': 'NaN â†’ 0',
        'reason': 'í™œë™/ê±°ë˜ ì—†ëŠ” ì‚¬ìš©ì = 0 (ì˜ë¯¸ ìˆëŠ” ì‹ í˜¸)'
    }
}


# ============================================
# ë°ì´í„° ë¡œë“œ í•¨ìˆ˜
# ============================================
def load_raw_data(data_dir: Optional[Path] = None) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """
    ì›ë³¸ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    
    Returns:
        train, user_logs, transactions, members ë°ì´í„°í”„ë ˆì„ íŠœí”Œ
    """
    if data_dir is None:
        data_dir = DATA_DIR
    
    print("ğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")
    
    train = pd.read_csv(data_dir / 'train_v2.csv')
    print(f"  âœ“ train_v2.csv: {len(train):,} rows")
    
    user_logs = pd.read_csv(data_dir / 'user_logs_v2.csv')
    print(f"  âœ“ user_logs_v2.csv: {len(user_logs):,} rows")
    
    transactions = pd.read_csv(data_dir / 'transactions_v2.csv')
    print(f"  âœ“ transactions_v2.csv: {len(transactions):,} rows")
    
    members = pd.read_csv(data_dir / 'members_v3.csv')
    print(f"  âœ“ members_v3.csv: {len(members):,} rows")
    
    return train, user_logs, transactions, members


# ============================================
# ì „ì²˜ë¦¬ í•¨ìˆ˜
# ============================================
def preprocess_dates(user_logs: pd.DataFrame, 
                     transactions: pd.DataFrame, 
                     members: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """
    ë‚ ì§œ ì»¬ëŸ¼ì„ datetime í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    print("\nğŸ”§ ë‚ ì§œ í˜•ì‹ ë³€í™˜ ì¤‘...")
    
    # user_logs
    user_logs = user_logs.copy()
    user_logs['date'] = pd.to_datetime(user_logs['date'], format='%Y%m%d')
    
    # transactions
    transactions = transactions.copy()
    transactions['transaction_date'] = pd.to_datetime(transactions['transaction_date'], format='%Y%m%d')
    transactions['membership_expire_date'] = pd.to_datetime(transactions['membership_expire_date'], format='%Y%m%d')
    
    # members
    members = members.copy()
    members['registration_init_time'] = pd.to_datetime(members['registration_init_time'], format='%Y%m%d')
    
    print("  âœ“ ë‚ ì§œ ë³€í™˜ ì™„ë£Œ")
    
    return user_logs, transactions, members


def filter_observation_window(user_logs: pd.DataFrame, 
                               transactions: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    ê´€ì¸¡ ìœˆë„ìš° ë° ì˜ˆì¸¡ ì‹œì  T ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„°ë¥¼ í•„í„°ë§í•©ë‹ˆë‹¤.
    - user_logs: 2017-03-01 ~ 2017-03-31
    - transactions: T (2017-04-01) ì´ì „ (ì „ì²´ ì´ë ¥ ì‚¬ìš©)
    """
    print("\nğŸ”§ ê´€ì¸¡ ìœˆë„ìš° í•„í„°ë§ ì¤‘...")
    
    # user_logs: ê´€ì¸¡ ìœˆë„ìš° ë‚´ ë°ì´í„°ë§Œ (30ì¼)
    user_logs_filtered = user_logs[
        (user_logs['date'] >= OBSERVATION_START) & 
        (user_logs['date'] <= OBSERVATION_END)
    ].copy()
    print(f"  âœ“ user_logs: {len(user_logs):,} â†’ {len(user_logs_filtered):,} rows (30ì¼ ìœˆë„ìš°)")
    
    # transactions: T ì´ì „ ë°ì´í„°ë§Œ (2015ë…„~2017ë…„ 3ì›”, ì•½ 2ë…„ì¹˜)
    transactions_filtered = transactions[
        transactions['transaction_date'] < PREDICTION_TIME
    ].copy()
    
    # transactions ê¸°ê°„ í™•ì¸
    txn_min = transactions_filtered['transaction_date'].min()
    txn_max = transactions_filtered['transaction_date'].max()
    print(f"  âœ“ transactions: {len(transactions):,} â†’ {len(transactions_filtered):,} rows")
    print(f"    (ê¸°ê°„: {txn_min.strftime('%Y-%m-%d')} ~ {txn_max.strftime('%Y-%m-%d')})")
    
    return user_logs_filtered, transactions_filtered


# ============================================
# Feature Engineering í•¨ìˆ˜
# ============================================
def create_user_log_features(user_logs: pd.DataFrame) -> pd.DataFrame:
    """
    user_logs_v2ì—ì„œ ì‚¬ìš©ìë³„ í–‰ë™ í”¼ì²˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    ìƒì„±ë˜ëŠ” í”¼ì²˜:
    - total_songs: ì´ ì¬ìƒ ê³¡ ìˆ˜
    - total_secs: ì´ ì²­ì·¨ ì‹œê°„ (ì´ˆ)
    - num_25_sum: 25% ë¯¸ë§Œ ì²­ì·¨ ê³¡ ìˆ˜ (ìŠ¤í‚µ)
    - num_100_sum: ì™„ì£¼ ê³¡ ìˆ˜
    - num_unq_sum: ê³ ìœ  ê³¡ ìˆ˜
    - active_days: í™œë™ ì¼ìˆ˜
    - skip_ratio: ìŠ¤í‚µìœ¨
    - complete_ratio: ì™„ì£¼ìœ¨
    - avg_songs_per_day: ì¼í‰ê·  ì¬ìƒ ê³¡ ìˆ˜
    - avg_secs_per_day: ì¼í‰ê·  ì²­ì·¨ ì‹œê°„
    - listening_variety: ì²­ì·¨ ë‹¤ì–‘ì„±
    """
    print("\nğŸµ User Log Features ìƒì„± ì¤‘...")
    
    df = user_logs.copy()
    
    # ì´ ê³¡ ìˆ˜ ê³„ì‚°
    df['total_songs'] = (df['num_25'] + df['num_50'] + df['num_75'] + 
                         df['num_985'] + df['num_100'])
    
    # ì§‘ê³„
    agg_dict = {
        'total_songs': 'sum',
        'total_secs': 'sum',
        'num_25': 'sum',
        'num_50': 'sum',
        'num_75': 'sum',
        'num_985': 'sum',
        'num_100': 'sum',
        'num_unq': 'sum',
        'date': 'nunique'
    }
    
    features = df.groupby('msno').agg(agg_dict).reset_index()
    features.columns = ['msno', 'total_songs', 'total_secs', 'num_25_sum', 
                        'num_50_sum', 'num_75_sum', 'num_985_sum', 'num_100_sum',
                        'num_unq_sum', 'active_days']
    
    # íŒŒìƒ í”¼ì²˜ ìƒì„±
    eps = 1e-9  # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
    
    features['skip_ratio'] = features['num_25_sum'] / (features['total_songs'] + eps)
    features['complete_ratio'] = features['num_100_sum'] / (features['total_songs'] + eps)
    features['partial_ratio'] = (features['num_50_sum'] + features['num_75_sum']) / (features['total_songs'] + eps)
    features['avg_songs_per_day'] = features['total_songs'] / (features['active_days'] + eps)
    features['avg_secs_per_day'] = features['total_secs'] / (features['active_days'] + eps)
    features['listening_variety'] = features['num_unq_sum'] / (features['total_songs'] + eps)
    features['avg_song_length'] = features['total_secs'] / (features['total_songs'] + eps)
    
    print(f"  âœ“ {len(features):,} users, {len(features.columns)-1} features")
    
    return features


def create_transaction_features(transactions: pd.DataFrame) -> pd.DataFrame:
    """
    transactions_v2ì—ì„œ ì‚¬ìš©ìë³„ ê²°ì œ í”¼ì²˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    ì•½ 2ë…„ì¹˜ ê±°ë˜ ì´ë ¥ì„ ì§‘ê³„í•©ë‹ˆë‹¤.
    
    ìƒì„±ë˜ëŠ” í”¼ì²˜:
    - transaction_count: ê±°ë˜ íšŸìˆ˜
    - total_payment: ì´ ê²°ì œ ê¸ˆì•¡
    - avg_payment: í‰ê·  ê²°ì œ ê¸ˆì•¡
    - cancel_count: ì·¨ì†Œ íšŸìˆ˜
    - auto_renew_rate: ìë™ ê°±ì‹  ë¹„ìœ¨
    - is_auto_renew_last: ë§ˆì§€ë§‰ ê±°ë˜ ìë™ ê°±ì‹  ì—¬ë¶€
    - plan_days_last: ë§ˆì§€ë§‰ êµ¬ë… ê¸°ê°„
    - days_to_expire: ë§Œë£Œê¹Œì§€ ë‚¨ì€ ì¼ìˆ˜
    - discount_rate: í‰ê·  í• ì¸ìœ¨
    """
    print("\nğŸ’³ Transaction Features ìƒì„± ì¤‘...")
    
    df = transactions.copy()
    
    # ìµœì‹  ê±°ë˜ ì¶”ì¶œ
    df_sorted = df.sort_values(['msno', 'transaction_date'], ascending=[True, False])
    latest = df_sorted.groupby('msno').first().reset_index()
    
    # í• ì¸ìœ¨ ê³„ì‚°
    df['discount_rate'] = 1 - (df['actual_amount_paid'] / (df['plan_list_price'] + 1e-9))
    df['discount_rate'] = df['discount_rate'].clip(0, 1)  # 0~1 ë²”ìœ„ë¡œ ì œí•œ
    
    # ì§‘ê³„
    agg_dict = {
        'actual_amount_paid': ['sum', 'mean'],
        'plan_list_price': 'mean',
        'is_cancel': 'sum',
        'is_auto_renew': 'mean',
        'discount_rate': 'mean',
        'transaction_date': 'count'
    }
    
    features = df.groupby('msno').agg(agg_dict).reset_index()
    features.columns = ['msno', 'total_payment', 'avg_payment', 'avg_list_price',
                        'cancel_count', 'auto_renew_rate', 'avg_discount_rate', 
                        'transaction_count']
    
    # ìµœì‹  ê±°ë˜ ì •ë³´ ë³‘í•©
    latest_cols = latest[['msno', 'is_auto_renew', 'payment_plan_days', 
                          'membership_expire_date', 'payment_method_id']]
    latest_cols = latest_cols.rename(columns={
        'is_auto_renew': 'is_auto_renew_last',
        'payment_plan_days': 'plan_days_last',
        'membership_expire_date': 'expire_date',
        'payment_method_id': 'payment_method_last'
    })
    
    features = features.merge(latest_cols, on='msno', how='left')
    
    # ë§Œë£Œê¹Œì§€ ë‚¨ì€ ì¼ìˆ˜
    features['days_to_expire'] = (features['expire_date'] - PREDICTION_TIME).dt.days
    features = features.drop('expire_date', axis=1)
    
    # ì·¨ì†Œ ì—¬ë¶€ í”Œë˜ê·¸
    features['has_cancelled'] = (features['cancel_count'] > 0).astype(int)
    
    print(f"  âœ“ {len(features):,} users, {len(features.columns)-1} features")
    
    return features


def create_member_features(members: pd.DataFrame) -> pd.DataFrame:
    """
    members_v3ì—ì„œ ì‚¬ìš©ìë³„ ì •ì  í”¼ì²˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    ìƒì„±ë˜ëŠ” í”¼ì²˜:
    - tenure_days: ê°€ì… í›„ ê²½ê³¼ ì¼ìˆ˜
    - city: ë„ì‹œ ì½”ë“œ
    - age: ë‚˜ì´ (ì´ìƒì¹˜ ì²˜ë¦¬ë¨)
    - gender: ì„±ë³„
    - registered_via: ê°€ì… ê²½ë¡œ
    """
    print("\nğŸ‘¤ Member Features ìƒì„± ì¤‘...")
    
    df = members.copy()
    
    # ê°€ì… í›„ ê²½ê³¼ ì¼ìˆ˜
    df['tenure_days'] = (PREDICTION_TIME - df['registration_init_time']).dt.days
    
    # ë‚˜ì´ ì´ìƒì¹˜ ì²˜ë¦¬ (0~100 ë²”ìœ„ ì™¸ â†’ NaN â†’ ì¤‘ì•™ê°’ ëŒ€ì²´)
    original_invalid = ((df['bd'] <= 0) | (df['bd'] >= 100)).sum()
    df['bd'] = df['bd'].apply(lambda x: x if 0 < x < 100 else np.nan)
    median_age = df['bd'].median()
    df['bd'] = df['bd'].fillna(median_age)
    df = df.rename(columns={'bd': 'age'})
    print(f"  âœ“ ë‚˜ì´ ì´ìƒì¹˜ {original_invalid:,}ê°œ â†’ ì¤‘ì•™ê°’({median_age:.0f})ìœ¼ë¡œ ëŒ€ì²´")
    
    # ì„±ë³„ ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    gender_missing = df['gender'].isnull().sum()
    df['gender'] = df['gender'].fillna('unknown')
    print(f"  âœ“ ì„±ë³„ ê²°ì¸¡ì¹˜ {gender_missing:,}ê°œ â†’ 'unknown'ìœ¼ë¡œ ëŒ€ì²´")
    
    # ë„ì‹œ ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    df['city'] = df['city'].fillna(0).astype(int)
    
    # ê°€ì… ê²½ë¡œ ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    df['registered_via'] = df['registered_via'].fillna(0).astype(int)
    
    # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
    features = df[['msno', 'city', 'age', 'gender', 'registered_via', 'tenure_days']]
    
    print(f"  âœ“ {len(features):,} users, {len(features.columns)-1} features")
    
    return features


# ============================================
# ì¸ì½”ë”© í•¨ìˆ˜
# ============================================
def encode_categorical_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    ë²”ì£¼í˜• ë³€ìˆ˜ë¥¼ ì¸ì½”ë”©í•©ë‹ˆë‹¤.
    - gender: One-hot encoding
    - city, registered_via, payment_method_last: ê·¸ëŒ€ë¡œ ìœ ì§€ (ìˆ˜ì¹˜í˜•ìœ¼ë¡œ ì²˜ë¦¬)
    """
    print("\nğŸ”¢ ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”© ì¤‘...")
    
    df = df.copy()
    
    # gender One-hot encoding
    if 'gender' in df.columns:
        gender_dummies = pd.get_dummies(df['gender'], prefix='gender')
        df = pd.concat([df.drop('gender', axis=1), gender_dummies], axis=1)
        print(f"  âœ“ gender â†’ One-hot ({gender_dummies.shape[1]} columns)")
    
    return df


# ============================================
# ë°ì´í„° ë³‘í•© í•¨ìˆ˜
# ============================================
def merge_features(train: pd.DataFrame,
                   user_log_features: pd.DataFrame,
                   transaction_features: pd.DataFrame,
                   member_features: pd.DataFrame) -> pd.DataFrame:
    """
    ëª¨ë“  í”¼ì²˜ë¥¼ train ê¸°ì¤€ìœ¼ë¡œ LEFT JOINí•˜ì—¬ ë³‘í•©í•©ë‹ˆë‹¤.
    """
    print("\nğŸ”— í”¼ì²˜ ë³‘í•© ì¤‘...")
    
    # train ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©
    df = train.copy()
    
    df = df.merge(user_log_features, on='msno', how='left')
    print(f"  âœ“ + user_log_features: {df.shape}")
    
    df = df.merge(transaction_features, on='msno', how='left')
    print(f"  âœ“ + transaction_features: {df.shape}")
    
    df = df.merge(member_features, on='msno', how='left')
    print(f"  âœ“ + member_features: {df.shape}")
    
    return df


def handle_missing_values(df: pd.DataFrame) -> pd.DataFrame:
    """
    ê²°ì¸¡ì¹˜ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    - ìˆ˜ì¹˜í˜•: 0ìœ¼ë¡œ ì±„ì›€ (í™œë™ ì—†ìŒ = 0)
    - ë²”ì£¼í˜•: 'unknown' ë˜ëŠ” ìµœë¹ˆê°’
    """
    print("\nğŸ”§ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì¤‘...")
    
    df = df.copy()
    
    # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ê²°ì¸¡ì¹˜ â†’ 0
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    numeric_cols = [c for c in numeric_cols if c not in ['msno', 'is_churn']]
    
    missing_before = df[numeric_cols].isnull().sum().sum()
    df[numeric_cols] = df[numeric_cols].fillna(0)
    print(f"  âœ“ ìˆ˜ì¹˜í˜• ê²°ì¸¡ì¹˜ {missing_before:,}ê°œ â†’ 0ìœ¼ë¡œ ëŒ€ì²´")
    
    # ë²”ì£¼í˜• ì»¬ëŸ¼ ê²°ì¸¡ì¹˜
    if 'gender' in df.columns:
        df['gender'] = df['gender'].fillna('unknown')
    
    return df


# ============================================
# ë°ì´í„° ë¶„í•  í•¨ìˆ˜
# ============================================
def split_dataset(df: pd.DataFrame, 
                  test_size: float = 0.15, 
                  valid_size: float = 0.15,
                  random_state: int = 719) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """
    ë°ì´í„°ë¥¼ train/valid/testë¡œ ë¶„í• í•©ë‹ˆë‹¤.
    Stratified splitìœ¼ë¡œ churn ë¹„ìœ¨ì„ ìœ ì§€í•©ë‹ˆë‹¤.
    
    Args:
        df: ì „ì²´ ë°ì´í„°ì…‹
        test_size: í…ŒìŠ¤íŠ¸ì…‹ ë¹„ìœ¨ (default: 0.2)
        valid_size: ê²€ì¦ì…‹ ë¹„ìœ¨ (default: 0.1)
        random_state: ëœë¤ ì‹œë“œ
    
    Returns:
        train, valid, test ë°ì´í„°í”„ë ˆì„ íŠœí”Œ
        
    ë¶„í•  ë¹„ìœ¨:
        - train: 70%
        - valid: 10%
        - test: 20%
    """
    print("\nâœ‚ï¸ ë°ì´í„°ì…‹ ë¶„í•  ì¤‘...")
    
    # ì²« ë²ˆì§¸ ë¶„í• : train+valid / test
    train_valid, test = train_test_split(
        df, 
        test_size=test_size, 
        stratify=df['is_churn'], 
        random_state=random_state
    )
    
    # ë‘ ë²ˆì§¸ ë¶„í• : train / valid
    valid_ratio = valid_size / (1 - test_size)  # 0.1 / 0.8 = 0.125
    train, valid = train_test_split(
        train_valid, 
        test_size=valid_ratio, 
        stratify=train_valid['is_churn'], 
        random_state=random_state
    )
    
    print(f"  âœ“ Train: {len(train):,} rows ({len(train)/len(df)*100:.1f}%)")
    print(f"  âœ“ Valid: {len(valid):,} rows ({len(valid)/len(df)*100:.1f}%)")
    print(f"  âœ“ Test:  {len(test):,} rows ({len(test)/len(df)*100:.1f}%)")
    
    # Churn ë¹„ìœ¨ í™•ì¸
    print(f"\n  Churn ë¹„ìœ¨:")
    print(f"    - Train: {train['is_churn'].mean()*100:.2f}%")
    print(f"    - Valid: {valid['is_churn'].mean()*100:.2f}%")
    print(f"    - Test:  {test['is_churn'].mean()*100:.2f}%")
    
    return train, valid, test


# ============================================
# Sanity Check í•¨ìˆ˜
# ============================================
def sanity_check(df: pd.DataFrame, name: str = "Dataset") -> Dict[str, Any]:
    """
    ì „ì²˜ë¦¬ ê²°ê³¼ë¥¼ ê²€ì¦í•©ë‹ˆë‹¤.
    
    ê²€ì¦ í•­ëª©:
    1. ë°ì´í„° shape
    2. ê²°ì¸¡ì¹˜ ê°œìˆ˜
    3. ì¤‘ë³µ msno ì—¬ë¶€
    4. Churn ë¹„ìœ¨
    5. ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ìŒìˆ˜ê°’ ì—¬ë¶€
    6. ê° ì»¬ëŸ¼ ê¸°ì´ˆ í†µê³„
    """
    print(f"\nğŸ” Sanity Check: {name}")
    print("-" * 50)
    
    results = {}
    
    # 1. Shape
    results['shape'] = df.shape
    print(f"  Shape: {df.shape[0]:,} rows Ã— {df.shape[1]} columns")
    
    # 2. ê²°ì¸¡ì¹˜
    missing_total = df.isnull().sum().sum()
    missing_cols = df.isnull().sum()
    missing_cols = missing_cols[missing_cols > 0]
    results['missing_total'] = missing_total
    print(f"  ê²°ì¸¡ì¹˜: {missing_total:,}ê°œ")
    if len(missing_cols) > 0:
        print(f"    - ê²°ì¸¡ ì»¬ëŸ¼: {dict(missing_cols)}")
    
    # 3. ì¤‘ë³µ msno
    duplicates = df['msno'].duplicated().sum()
    results['duplicates'] = duplicates
    print(f"  ì¤‘ë³µ msno: {duplicates:,}ê°œ")
    if duplicates > 0:
        print("    âš ï¸ ê²½ê³ : ì¤‘ë³µëœ ì‚¬ìš©ìê°€ ìˆìŠµë‹ˆë‹¤!")
    
    # 4. Churn ë¹„ìœ¨
    if 'is_churn' in df.columns:
        churn_rate = df['is_churn'].mean()
        results['churn_rate'] = churn_rate
        print(f"  Churn ë¹„ìœ¨: {churn_rate*100:.2f}%")
    
    # 5. ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ìŒìˆ˜ê°’ ì²´í¬
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    negative_check = {}
    for col in numeric_cols:
        if col in ['is_churn', 'msno']:
            continue
        neg_count = (df[col] < 0).sum()
        if neg_count > 0:
            negative_check[col] = neg_count
    
    results['negative_values'] = negative_check
    if negative_check:
        print(f"  ìŒìˆ˜ê°’ ì»¬ëŸ¼: {negative_check}")
    else:
        print(f"  ìŒìˆ˜ê°’: ì—†ìŒ âœ“")
    
    # 6. ë¬´í•œê°’ ì²´í¬
    inf_check = {}
    for col in numeric_cols:
        inf_count = np.isinf(df[col]).sum()
        if inf_count > 0:
            inf_check[col] = inf_count
    
    results['infinite_values'] = inf_check
    if inf_check:
        print(f"  ë¬´í•œê°’ ì»¬ëŸ¼: {inf_check}")
    else:
        print(f"  ë¬´í•œê°’: ì—†ìŒ âœ“")
    
    print("-" * 50)
    
    return results


# ============================================
# ë©”ì¸ íŒŒì´í”„ë¼ì¸
# ============================================
def run_preprocessing_pipeline(data_dir: Optional[Path] = None, 
                                save_dir: Optional[Path] = None,
                                split_data: bool = True) -> Tuple[pd.DataFrame, Optional[Tuple]]:
    """
    ì „ì²´ ì „ì²˜ë¦¬ ë° í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
    
    Args:
        data_dir: ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
        save_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ (Noneì´ë©´ ì €ì¥í•˜ì§€ ì•ŠìŒ)
        split_data: train/valid/test ë¶„í•  ì—¬ë¶€
    
    Returns:
        (ì „ì²´ í”¼ì²˜ í…Œì´ë¸”, (train, valid, test) ë˜ëŠ” None)
    """
    print("=" * 60)
    print("ğŸš€ KKBox Preprocessing & Feature Engineering Pipeline")
    print("=" * 60)
    print(f"ì˜ˆì¸¡ ì‹œì  (T): {PREDICTION_TIME.strftime('%Y-%m-%d')}")
    print(f"ê´€ì¸¡ ìœˆë„ìš°: {OBSERVATION_START.strftime('%Y-%m-%d')} ~ {OBSERVATION_END.strftime('%Y-%m-%d')}")
    
    # 1. ë°ì´í„° ë¡œë“œ
    train, user_logs, transactions, members = load_raw_data(data_dir)
    
    # 2. ë‚ ì§œ ì „ì²˜ë¦¬
    user_logs, transactions, members = preprocess_dates(user_logs, transactions, members)
    
    # 3. ê´€ì¸¡ ìœˆë„ìš° í•„í„°ë§
    user_logs, transactions = filter_observation_window(user_logs, transactions)
    
    # 4. Feature Engineering
    user_log_features = create_user_log_features(user_logs)
    transaction_features = create_transaction_features(transactions)
    member_features = create_member_features(members)
    
    # 5. ë°ì´í„° ë³‘í•©
    df = merge_features(train, user_log_features, transaction_features, member_features)
    
    # 6. ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    df = handle_missing_values(df)
    
    # 7. ë²”ì£¼í˜• ì¸ì½”ë”©
    df = encode_categorical_features(df)
    
    # 8. Sanity Check (ì „ì²´ ë°ì´í„°)
    sanity_check(df, "Full Dataset")
    
    # 9. ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 60)
    print("âœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
    print("=" * 60)
    print(f"ìµœì¢… ë°ì´í„°ì…‹: {df.shape[0]:,} rows Ã— {df.shape[1]} columns")
    print(f"Churn ë¹„ìœ¨: {df['is_churn'].mean()*100:.2f}%")
    
    # 10. ë°ì´í„° ë¶„í• 
    splits = None
    if split_data:
        train_df, valid_df, test_df = split_dataset(df)
        splits = (train_df, valid_df, test_df)
        
        # ë¶„í•  ë°ì´í„° Sanity Check
        sanity_check(train_df, "Train Set")
        sanity_check(valid_df, "Valid Set")
        sanity_check(test_df, "Test Set")
    
    # 11. ì €ì¥ (ì˜µì…˜)
    if save_dir:
        save_dir = Path(save_dir)
        save_dir.mkdir(parents=True, exist_ok=True)
        
        # ì „ì²´ ë°ì´í„° ì €ì¥
        df.to_csv(save_dir / 'feature_table_ldh.csv', index=False)
        print(f"\nğŸ’¾ ì €ì¥ ì™„ë£Œ: {save_dir / 'feature_table_ldh.csv'}")
        
        # ë¶„í•  ë°ì´í„° ì €ì¥
        if splits:
            train_df, valid_df, test_df = splits
            train_df.to_csv(save_dir / 'train_set.csv', index=False)
            valid_df.to_csv(save_dir / 'valid_set.csv', index=False)
            test_df.to_csv(save_dir / 'test_set.csv', index=False)
            print(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: train_set.csv, valid_set.csv, test_set.csv")
    
    return df, splits


def get_feature_info() -> pd.DataFrame:
    """
    ìƒì„±ëœ í”¼ì²˜ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    feature_info = [
        # User Log Features
        {'feature': 'total_songs', 'type': 'numeric', 'source': 'user_logs_v2', 'description': '30ì¼ê°„ ì´ ì¬ìƒ ê³¡ ìˆ˜'},
        {'feature': 'total_secs', 'type': 'numeric', 'source': 'user_logs_v2', 'description': '30ì¼ê°„ ì´ ì²­ì·¨ ì‹œê°„ (ì´ˆ)'},
        {'feature': 'num_25_sum', 'type': 'numeric', 'source': 'user_logs_v2', 'description': '25% ë¯¸ë§Œ ì²­ì·¨ ê³¡ ìˆ˜ (ìŠ¤í‚µ)'},
        {'feature': 'num_50_sum', 'type': 'numeric', 'source': 'user_logs_v2', 'description': '25-50% ì²­ì·¨ ê³¡ ìˆ˜'},
        {'feature': 'num_75_sum', 'type': 'numeric', 'source': 'user_logs_v2', 'description': '50-75% ì²­ì·¨ ê³¡ ìˆ˜'},
        {'feature': 'num_985_sum', 'type': 'numeric', 'source': 'user_logs_v2', 'description': '75-98.5% ì²­ì·¨ ê³¡ ìˆ˜'},
        {'feature': 'num_100_sum', 'type': 'numeric', 'source': 'user_logs_v2', 'description': '98.5%+ ì™„ì£¼ ê³¡ ìˆ˜'},
        {'feature': 'num_unq_sum', 'type': 'numeric', 'source': 'user_logs_v2', 'description': 'ê³ ìœ  ê³¡ ìˆ˜'},
        {'feature': 'active_days', 'type': 'numeric', 'source': 'user_logs_v2', 'description': '30ì¼ ì¤‘ í™œë™ ì¼ìˆ˜'},
        {'feature': 'skip_ratio', 'type': 'numeric', 'source': 'user_logs_v2', 'description': 'ìŠ¤í‚µìœ¨ (num_25/total)'},
        {'feature': 'complete_ratio', 'type': 'numeric', 'source': 'user_logs_v2', 'description': 'ì™„ì£¼ìœ¨ (num_100/total)'},
        {'feature': 'partial_ratio', 'type': 'numeric', 'source': 'user_logs_v2', 'description': 'ë¶€ë¶„ì²­ì·¨ìœ¨ ((num_50+num_75)/total)'},
        {'feature': 'avg_songs_per_day', 'type': 'numeric', 'source': 'user_logs_v2', 'description': 'ì¼í‰ê·  ì¬ìƒ ê³¡ ìˆ˜'},
        {'feature': 'avg_secs_per_day', 'type': 'numeric', 'source': 'user_logs_v2', 'description': 'ì¼í‰ê·  ì²­ì·¨ ì‹œê°„ (ì´ˆ)'},
        {'feature': 'listening_variety', 'type': 'numeric', 'source': 'user_logs_v2', 'description': 'ì²­ì·¨ ë‹¤ì–‘ì„± (unique/total)'},
        {'feature': 'avg_song_length', 'type': 'numeric', 'source': 'user_logs_v2', 'description': 'í‰ê·  ê³¡ ê¸¸ì´ (ì´ˆ)'},
        
        # Transaction Features
        {'feature': 'total_payment', 'type': 'numeric', 'source': 'transactions_v2', 'description': 'ì´ ê²°ì œ ê¸ˆì•¡ (2ë…„ ëˆ„ì )'},
        {'feature': 'avg_payment', 'type': 'numeric', 'source': 'transactions_v2', 'description': 'í‰ê·  ê²°ì œ ê¸ˆì•¡'},
        {'feature': 'avg_list_price', 'type': 'numeric', 'source': 'transactions_v2', 'description': 'í‰ê·  ì •ê°€'},
        {'feature': 'cancel_count', 'type': 'numeric', 'source': 'transactions_v2', 'description': 'ì·¨ì†Œ íšŸìˆ˜'},
        {'feature': 'auto_renew_rate', 'type': 'numeric', 'source': 'transactions_v2', 'description': 'ìë™ ê°±ì‹  ë¹„ìœ¨'},
        {'feature': 'avg_discount_rate', 'type': 'numeric', 'source': 'transactions_v2', 'description': 'í‰ê·  í• ì¸ìœ¨'},
        {'feature': 'transaction_count', 'type': 'numeric', 'source': 'transactions_v2', 'description': 'ê±°ë˜ íšŸìˆ˜ (2ë…„ ëˆ„ì )'},
        {'feature': 'is_auto_renew_last', 'type': 'binary', 'source': 'transactions_v2', 'description': 'ìµœê·¼ ê±°ë˜ ìë™ê°±ì‹  ì—¬ë¶€'},
        {'feature': 'plan_days_last', 'type': 'numeric', 'source': 'transactions_v2', 'description': 'ìµœê·¼ êµ¬ë… ê¸°ê°„ (ì¼)'},
        {'feature': 'payment_method_last', 'type': 'categorical', 'source': 'transactions_v2', 'description': 'ìµœê·¼ ê²°ì œ ìˆ˜ë‹¨'},
        {'feature': 'days_to_expire', 'type': 'numeric', 'source': 'transactions_v2', 'description': 'ë§Œë£Œê¹Œì§€ ë‚¨ì€ ì¼ìˆ˜ (T ê¸°ì¤€)'},
        {'feature': 'has_cancelled', 'type': 'binary', 'source': 'transactions_v2', 'description': 'ì·¨ì†Œ ì´ë ¥ ì—¬ë¶€'},
        
        # Member Features
        {'feature': 'city', 'type': 'categorical', 'source': 'members_v3', 'description': 'ë„ì‹œ ì½”ë“œ'},
        {'feature': 'age', 'type': 'numeric', 'source': 'members_v3', 'description': 'ë‚˜ì´ (ì´ìƒì¹˜ ì²˜ë¦¬ë¨)'},
        {'feature': 'registered_via', 'type': 'categorical', 'source': 'members_v3', 'description': 'ê°€ì… ê²½ë¡œ'},
        {'feature': 'tenure_days', 'type': 'numeric', 'source': 'members_v3', 'description': 'ê°€ì… í›„ ê²½ê³¼ ì¼ìˆ˜'},
        {'feature': 'gender_female', 'type': 'binary', 'source': 'members_v3', 'description': 'ì„±ë³„: ì—¬ì„±'},
        {'feature': 'gender_male', 'type': 'binary', 'source': 'members_v3', 'description': 'ì„±ë³„: ë‚¨ì„±'},
        {'feature': 'gender_unknown', 'type': 'binary', 'source': 'members_v3', 'description': 'ì„±ë³„: ë¯¸ì…ë ¥'},
    ]
    
    return pd.DataFrame(feature_info)


# ============================================
# ì‹¤í–‰
# ============================================
if __name__ == "__main__":
    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ë° ì €ì¥
    df, splits = run_preprocessing_pipeline(save_dir=DATA_DIR, split_data=True)
    
    # í”¼ì²˜ ëª©ë¡ ì¶œë ¥
    print("\nğŸ“‹ ìƒì„±ëœ í”¼ì²˜ ëª©ë¡:")
    for i, col in enumerate(df.columns):
        print(f"  {i+1:2d}. {col}")
    
    # í”¼ì²˜ ì •ë³´ ì €ì¥
    feature_df = get_feature_info()
    feature_df.to_csv(DATA_DIR / 'feature_dictionary.csv', index=False)
    print(f"\nğŸ’¾ í”¼ì²˜ ì‚¬ì „ ì €ì¥: {DATA_DIR / 'feature_dictionary.csv'}")
