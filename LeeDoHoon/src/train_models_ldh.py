"""
KKBox Churn Prediction - Model Training & Evaluation
ì‘ì„±ì: ì´ë„í›ˆ (LDH)
ì‘ì„±ì¼: 2025-12-16

ì´ ëª¨ë“ˆì€ ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ML ëª¨ë¸ì„ í•™ìŠµí•˜ê³  í‰ê°€í•©ë‹ˆë‹¤.
- Logistic Regression (Baseline)
- LightGBM (Tree-based)

í‰ê°€ ì§€í‘œ:
- ROC-AUC
- PR-AUC (Average Precision)
- Recall
- Precision
- F1-Score
"""

import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, Any, Tuple, Optional
import json
import warnings

# ML Libraries
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    roc_auc_score, 
    average_precision_score, 
    recall_score, 
    precision_score, 
    f1_score,
    confusion_matrix,
    classification_report,
    roc_curve,
    precision_recall_curve
)
import lightgbm as lgb
import joblib

warnings.filterwarnings('ignore')

# ============================================
# ì„¤ì •
# ============================================
DATA_DIR = Path(__file__).parent.parent / 'data'
MODEL_DIR = Path(__file__).parent.parent / 'models'
REPORT_DIR = Path(__file__).parent.parent / 'docs' / '02_training_report'

# ëœë¤ ì‹œë“œ
RANDOM_STATE = 719

# ì œì™¸í•  ì»¬ëŸ¼
EXCLUDE_COLS = ['msno', 'is_churn']


# ============================================
# ë°ì´í„° ë¡œë“œ í•¨ìˆ˜
# ============================================
def load_datasets(data_dir: Optional[Path] = None) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """
    ì „ì²˜ë¦¬ëœ train/valid/test ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    """
    if data_dir is None:
        data_dir = DATA_DIR
    
    print("ğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")
    
    train = pd.read_csv(data_dir / 'train_set.csv')
    valid = pd.read_csv(data_dir / 'valid_set.csv')
    test = pd.read_csv(data_dir / 'test_set.csv')
    
    print(f"  âœ“ Train: {train.shape}")
    print(f"  âœ“ Valid: {valid.shape}")
    print(f"  âœ“ Test:  {test.shape}")
    
    return train, valid, test


def prepare_features(train: pd.DataFrame, 
                     valid: pd.DataFrame, 
                     test: pd.DataFrame) -> Tuple:
    """
    í•™ìŠµì— ì‚¬ìš©í•  í”¼ì²˜ì™€ íƒ€ê²Ÿì„ ë¶„ë¦¬í•©ë‹ˆë‹¤.
    """
    # í”¼ì²˜ ì»¬ëŸ¼ ì„ íƒ
    feature_cols = [c for c in train.columns if c not in EXCLUDE_COLS]
    
    X_train = train[feature_cols]
    y_train = train['is_churn']
    
    X_valid = valid[feature_cols]
    y_valid = valid['is_churn']
    
    X_test = test[feature_cols]
    y_test = test['is_churn']
    
    print(f"\nğŸ“Š í”¼ì²˜ ì¤€ë¹„ ì™„ë£Œ")
    print(f"  í”¼ì²˜ ìˆ˜: {len(feature_cols)}")
    print(f"  Train Churn ë¹„ìœ¨: {y_train.mean()*100:.2f}%")
    print(f"  Valid Churn ë¹„ìœ¨: {y_valid.mean()*100:.2f}%")
    print(f"  Test Churn ë¹„ìœ¨: {y_test.mean()*100:.2f}%")
    
    return X_train, y_train, X_valid, y_valid, X_test, y_test, feature_cols


# ============================================
# í‰ê°€ í•¨ìˆ˜
# ============================================
def evaluate_model(y_true: np.ndarray, 
                   y_pred: np.ndarray, 
                   y_prob: np.ndarray,
                   threshold: float = 0.5) -> Dict[str, float]:
    """
    ëª¨ë¸ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.
    
    Returns:
        í‰ê°€ ì§€í‘œ ë”•ì…”ë„ˆë¦¬
    """
    # ì´ì§„ ì˜ˆì¸¡
    y_pred_binary = (y_prob >= threshold).astype(int)
    
    metrics = {
        'roc_auc': roc_auc_score(y_true, y_prob),
        'pr_auc': average_precision_score(y_true, y_prob),
        'recall': recall_score(y_true, y_pred_binary),
        'precision': precision_score(y_true, y_pred_binary),
        'f1': f1_score(y_true, y_pred_binary),
    }
    
    # Confusion Matrix
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred_binary).ravel()
    metrics['true_negative'] = int(tn)
    metrics['false_positive'] = int(fp)
    metrics['false_negative'] = int(fn)
    metrics['true_positive'] = int(tp)
    metrics['specificity'] = tn / (tn + fp) if (tn + fp) > 0 else 0
    
    return metrics


def print_metrics(metrics: Dict[str, float], name: str) -> None:
    """
    í‰ê°€ ì§€í‘œë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
    """
    print(f"\n[{name}] í‰ê°€ ê²°ê³¼:")
    print(f"  ROC-AUC:   {metrics['roc_auc']:.4f}")
    print(f"  PR-AUC:    {metrics['pr_auc']:.4f}")
    print(f"  Recall:    {metrics['recall']:.4f}")
    print(f"  Precision: {metrics['precision']:.4f}")
    print(f"  F1-Score:  {metrics['f1']:.4f}")
    print(f"  Specificity: {metrics['specificity']:.4f}")
    print(f"\n  Confusion Matrix:")
    print(f"    TP: {metrics['true_positive']:,}  FP: {metrics['false_positive']:,}")
    print(f"    FN: {metrics['false_negative']:,}  TN: {metrics['true_negative']:,}")


# ============================================
# Logistic Regression
# ============================================
def train_logistic_regression(X_train: pd.DataFrame,
                               y_train: pd.Series,
                               X_valid: pd.DataFrame,
                               y_valid: pd.Series) -> Tuple[Any, StandardScaler, Dict]:
    """
    Logistic Regression ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.
    """
    print("\n" + "=" * 60)
    print("Logistic Regression í•™ìŠµ")
    print("=" * 60)
    
    # ìŠ¤ì¼€ì¼ë§
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_valid_scaled = scaler.transform(X_valid)
    
    # ëª¨ë¸ ì •ì˜
    model = LogisticRegression(
        C=1.0,
        class_weight='balanced',  # í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬
        max_iter=1000,
        random_state=RANDOM_STATE,
        n_jobs=-1
    )
    
    # í•™ìŠµ
    print("  í•™ìŠµ ì¤‘...")
    model.fit(X_train_scaled, y_train)
    
    # ì˜ˆì¸¡
    y_train_prob = model.predict_proba(X_train_scaled)[:, 1]
    y_valid_prob = model.predict_proba(X_valid_scaled)[:, 1]
    
    # í‰ê°€
    train_metrics = evaluate_model(y_train, model.predict(X_train_scaled), y_train_prob)
    valid_metrics = evaluate_model(y_valid, model.predict(X_valid_scaled), y_valid_prob)
    
    print_metrics(train_metrics, "Train")
    print_metrics(valid_metrics, "Validation")
    
    results = {
        'model_name': 'Logistic Regression',
        'train_metrics': train_metrics,
        'valid_metrics': valid_metrics,
        'params': {
            'C': 1.0,
            'class_weight': 'balanced',
            'max_iter': 1000
        }
    }
    
    return model, scaler, results


# ============================================
# LightGBM
# ============================================
def train_lightgbm(X_train: pd.DataFrame,
                   y_train: pd.Series,
                   X_valid: pd.DataFrame,
                   y_valid: pd.Series) -> Tuple[Any, Dict]:
    """
    LightGBM ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.
    """
    print("\n" + "=" * 60)
    print("LightGBM í•™ìŠµ")
    print("=" * 60)
    
    # í´ë˜ìŠ¤ ë¶ˆê· í˜• ê³„ì‚°
    scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()
    print(f"  scale_pos_weight: {scale_pos_weight:.2f}")
    
    # íŒŒë¼ë¯¸í„° ì„¤ì •
    params = {
        'objective': 'binary',
        'metric': ['auc', 'binary_logloss'],
        'boosting_type': 'gbdt',
        'num_leaves': 31,
        'max_depth': 6,
        'learning_rate': 0.05,
        'feature_fraction': 0.8,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'scale_pos_weight': scale_pos_weight,
        'min_child_samples': 100,
        'reg_alpha': 0.1,
        'reg_lambda': 0.1,
        'random_state': RANDOM_STATE,
        'verbose': -1,
        'n_jobs': -1
    }
    
    # ë°ì´í„°ì…‹ ìƒì„±
    train_data = lgb.Dataset(X_train, label=y_train)
    valid_data = lgb.Dataset(X_valid, label=y_valid, reference=train_data)
    
    # í•™ìŠµ
    print("  í•™ìŠµ ì¤‘...")
    model = lgb.train(
        params,
        train_data,
        num_boost_round=500,
        valid_sets=[train_data, valid_data],
        valid_names=['train', 'valid'],
        callbacks=[
            lgb.early_stopping(stopping_rounds=50),
            lgb.log_evaluation(period=100)
        ]
    )
    
    # ì˜ˆì¸¡
    y_train_prob = model.predict(X_train, num_iteration=model.best_iteration)
    y_valid_prob = model.predict(X_valid, num_iteration=model.best_iteration)
    
    # í‰ê°€
    train_metrics = evaluate_model(y_train, (y_train_prob >= 0.5).astype(int), y_train_prob)
    valid_metrics = evaluate_model(y_valid, (y_valid_prob >= 0.5).astype(int), y_valid_prob)
    
    print_metrics(train_metrics, "Train")
    print_metrics(valid_metrics, "Validation")
    
    # Feature Importance
    importance = pd.DataFrame({
        'feature': X_train.columns,
        'importance': model.feature_importance(importance_type='gain')
    }).sort_values('importance', ascending=False)
    
    print("\nğŸ“Š Top 10 Feature Importance:")
    for i, row in importance.head(10).iterrows():
        print(f"  {row['feature']}: {row['importance']:.2f}")
    
    results = {
        'model_name': 'LightGBM',
        'train_metrics': train_metrics,
        'valid_metrics': valid_metrics,
        'params': params,
        'best_iteration': model.best_iteration,
        'feature_importance': importance.to_dict('records')
    }
    
    return model, results


# ============================================
# í…ŒìŠ¤íŠ¸ì…‹ ìµœì¢… í‰ê°€
# ============================================
def evaluate_on_test(models: Dict[str, Any],
                     X_test: pd.DataFrame,
                     y_test: pd.Series,
                     scaler: StandardScaler = None) -> Dict[str, Dict]:
    """
    í…ŒìŠ¤íŠ¸ì…‹ì—ì„œ ëª¨ë“  ëª¨ë¸ì„ í‰ê°€í•©ë‹ˆë‹¤.
    """
    print("\n" + "=" * 60)
    print("í…ŒìŠ¤íŠ¸ì…‹ ìµœì¢… í‰ê°€")
    print("=" * 60)
    
    test_results = {}
    
    for name, model in models.items():
        print(f"\n--- {name} ---")
        
        if name == 'Logistic Regression':
            X_test_scaled = scaler.transform(X_test)
            y_prob = model.predict_proba(X_test_scaled)[:, 1]
        else:
            y_prob = model.predict(X_test, num_iteration=model.best_iteration)
        
        metrics = evaluate_model(y_test, (y_prob >= 0.5).astype(int), y_prob)
        print_metrics(metrics, f"{name} (Test)")
        
        test_results[name] = metrics
    
    return test_results


# ============================================
# ê²°ê³¼ ì €ì¥
# ============================================
def save_results(all_results: Dict,
                 models: Dict,
                 scaler: StandardScaler,
                 model_dir: Path,
                 feature_cols: list) -> None:
    """
    í•™ìŠµ ê²°ê³¼ì™€ ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤.
    """
    model_dir.mkdir(parents=True, exist_ok=True)
    
    print("\nëª¨ë¸ ì €ì¥ ì¤‘...")
    
    # Logistic Regression ì €ì¥
    joblib.dump(models['Logistic Regression'], model_dir / 'logistic_regression.pkl')
    joblib.dump(scaler, model_dir / 'scaler.pkl')
    print(f"  âœ“ Logistic Regression ì €ì¥: {model_dir / 'logistic_regression.pkl'}")
    
    # LightGBM ì €ì¥
    models['LightGBM'].save_model(str(model_dir / 'lightgbm.txt'))
    print(f"  âœ“ LightGBM ì €ì¥: {model_dir / 'lightgbm.txt'}")
    
    # í”¼ì²˜ ëª©ë¡ ì €ì¥
    with open(model_dir / 'feature_cols.json', 'w') as f:
        json.dump(feature_cols, f)
    print(f"  âœ“ Feature ëª©ë¡ ì €ì¥: {model_dir / 'feature_cols.json'}")
    
    # ê²°ê³¼ JSON ì €ì¥
    # numpy/pandas íƒ€ì…ì„ Python ê¸°ë³¸ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
    def convert_types(obj):
        if isinstance(obj, (np.integer, np.floating)):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, dict):
            return {k: convert_types(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_types(i) for i in obj]
        return obj
    
    results_to_save = convert_types(all_results)
    with open(model_dir / 'training_results.json', 'w') as f:
        json.dump(results_to_save, f, indent=2)
    print(f"  âœ“ ê²°ê³¼ ì €ì¥: {model_dir / 'training_results.json'}")


def generate_report(all_results: Dict, report_dir: Path) -> None:
    """
    ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    report_dir.mkdir(parents=True, exist_ok=True)
    
    lr_valid = all_results['Logistic Regression']['valid_metrics']
    lr_test = all_results['Logistic Regression']['test_metrics']
    lgb_valid = all_results['LightGBM']['valid_metrics']
    lgb_test = all_results['LightGBM']['test_metrics']
    
    # Feature Importance (LightGBM)
    feature_imp = all_results['LightGBM'].get('feature_importance', [])[:10]
    
    report = f"""# 01. ML ëª¨ë¸ í•™ìŠµ ê²°ê³¼ (ML Training Results)

> **ì‘ì„±ì**: ì´ë„í›ˆ (LDH)  
> **ì‘ì„±ì¼**: 2025-12-16  
> **ë²„ì „**: v1.0

---

## 1. í•™ìŠµ ê°œìš”

### 1.1 ëª¨ë¸ ëª©ë¡
| ëª¨ë¸ | ìœ í˜• | ëª©ì  |
|------|------|------|
| Logistic Regression | Linear | Baseline ëª¨ë¸ |
| LightGBM | Tree-based | ì„±ëŠ¥ í–¥ìƒ ëª¨ë¸ |

### 1.2 ë°ì´í„° ë¶„í• 
| ì…‹ | ë¹„ìœ¨ | ìš©ë„ |
|----|------|------|
| Train | 70% | ëª¨ë¸ í•™ìŠµ |
| Valid | 10% | í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ / Early Stopping |
| Test | 20% | ìµœì¢… ì„±ëŠ¥ í‰ê°€ |

### 1.3 í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬
- **Logistic Regression**: `class_weight='balanced'`
- **LightGBM**: `scale_pos_weight` ì ìš©

---

## 2. í‰ê°€ ì§€í‘œ ë¹„êµ

### 2.1 Validation Set ì„±ëŠ¥

| ì§€í‘œ | Logistic Regression | LightGBM | ìš°ìˆ˜ ëª¨ë¸ |
|------|---------------------|----------|-----------|
| **ROC-AUC** | {lr_valid['roc_auc']:.4f} | {lgb_valid['roc_auc']:.4f} | {'LightGBM âœ…' if lgb_valid['roc_auc'] > lr_valid['roc_auc'] else 'Logistic âœ…'} |
| **PR-AUC** | {lr_valid['pr_auc']:.4f} | {lgb_valid['pr_auc']:.4f} | {'LightGBM âœ…' if lgb_valid['pr_auc'] > lr_valid['pr_auc'] else 'Logistic âœ…'} |
| **Recall** | {lr_valid['recall']:.4f} | {lgb_valid['recall']:.4f} | {'LightGBM âœ…' if lgb_valid['recall'] > lr_valid['recall'] else 'Logistic âœ…'} |
| **Precision** | {lr_valid['precision']:.4f} | {lgb_valid['precision']:.4f} | {'LightGBM âœ…' if lgb_valid['precision'] > lr_valid['precision'] else 'Logistic âœ…'} |
| **F1-Score** | {lr_valid['f1']:.4f} | {lgb_valid['f1']:.4f} | {'LightGBM âœ…' if lgb_valid['f1'] > lr_valid['f1'] else 'Logistic âœ…'} |

### 2.2 Test Set ì„±ëŠ¥ (ìµœì¢…)

| ì§€í‘œ | Logistic Regression | LightGBM | ìš°ìˆ˜ ëª¨ë¸ |
|------|---------------------|----------|-----------|
| **ROC-AUC** | {lr_test['roc_auc']:.4f} | {lgb_test['roc_auc']:.4f} | {'LightGBM âœ…' if lgb_test['roc_auc'] > lr_test['roc_auc'] else 'Logistic âœ…'} |
| **PR-AUC** | {lr_test['pr_auc']:.4f} | {lgb_test['pr_auc']:.4f} | {'LightGBM âœ…' if lgb_test['pr_auc'] > lr_test['pr_auc'] else 'Logistic âœ…'} |
| **Recall** | {lr_test['recall']:.4f} | {lgb_test['recall']:.4f} | {'LightGBM âœ…' if lgb_test['recall'] > lr_test['recall'] else 'Logistic âœ…'} |
| **Precision** | {lr_test['precision']:.4f} | {lgb_test['precision']:.4f} | {'LightGBM âœ…' if lgb_test['precision'] > lr_test['precision'] else 'Logistic âœ…'} |
| **F1-Score** | {lr_test['f1']:.4f} | {lgb_test['f1']:.4f} | {'LightGBM âœ…' if lgb_test['f1'] > lr_test['f1'] else 'Logistic âœ…'} |

---

## 3. Confusion Matrix (Test Set)

### 3.1 Logistic Regression

```
              Predicted
              0        1
Actual  0    {lr_test['true_negative']:,}    {lr_test['false_positive']:,}
        1    {lr_test['false_negative']:,}    {lr_test['true_positive']:,}
```

### 3.2 LightGBM

```
              Predicted
              0        1
Actual  0    {lgb_test['true_negative']:,}    {lgb_test['false_positive']:,}
        1    {lgb_test['false_negative']:,}    {lgb_test['true_positive']:,}
```

---

## 4. Feature Importance (LightGBM)

| ìˆœìœ„ | Feature | Importance |
|------|---------|------------|
"""
    
    for i, feat in enumerate(feature_imp, 1):
        report += f"| {i} | `{feat['feature']}` | {feat['importance']:.2f} |\n"
    
    report += f"""
---

## 5. ëª¨ë¸ë³„ í•˜ì´í¼íŒŒë¼ë¯¸í„°

### 5.1 Logistic Regression

| íŒŒë¼ë¯¸í„° | ê°’ |
|----------|-----|
| C (ê·œì œ ê°•ë„) | 1.0 |
| class_weight | balanced |
| max_iter | 1000 |
| solver | lbfgs |

### 5.2 LightGBM

| íŒŒë¼ë¯¸í„° | ê°’ |
|----------|-----|
| num_leaves | 31 |
| max_depth | 6 |
| learning_rate | 0.05 |
| feature_fraction | 0.8 |
| bagging_fraction | 0.8 |
| min_child_samples | 100 |
| reg_alpha | 0.1 |
| reg_lambda | 0.1 |
| best_iteration | {all_results['LightGBM'].get('best_iteration', 'N/A')} |

---

## 6. ê²°ë¡ 

### 6.1 ìµœì¢… ëª¨ë¸ ì„ ì •
- **ì¶”ì²œ ëª¨ë¸**: {'LightGBM' if lgb_test['roc_auc'] > lr_test['roc_auc'] else 'Logistic Regression'}
- **ì„ ì • ì‚¬ìœ **: ROC-AUC ê¸°ì¤€ ìš°ìˆ˜í•œ ì„±ëŠ¥

### 6.2 ì„±ëŠ¥ ìš”ì•½
- **ROC-AUC**: {max(lgb_test['roc_auc'], lr_test['roc_auc']):.4f}
- **PR-AUC**: {max(lgb_test['pr_auc'], lr_test['pr_auc']):.4f}
- **Recall**: {max(lgb_test['recall'], lr_test['recall']):.4f}

### 6.3 ì£¼ìš” ì´íƒˆ ì˜ˆì¸¡ í”¼ì²˜
1. **`{feature_imp[0]['feature'] if feature_imp else 'N/A'}`**: ê°€ì¥ ì¤‘ìš”í•œ ì´íƒˆ ì‹ í˜¸
2. **`{feature_imp[1]['feature'] if len(feature_imp) > 1 else 'N/A'}`**: ë‘ ë²ˆì§¸ ì¤‘ìš” í”¼ì²˜
3. **`{feature_imp[2]['feature'] if len(feature_imp) > 2 else 'N/A'}`**: ì„¸ ë²ˆì§¸ ì¤‘ìš” í”¼ì²˜

---

## 7. ì €ì¥ëœ íŒŒì¼

| íŒŒì¼ | ê²½ë¡œ | ì„¤ëª… |
|------|------|------|
| Logistic Regression | `models/logistic_regression.pkl` | Baseline ëª¨ë¸ |
| LightGBM | `models/lightgbm.txt` | Tree ëª¨ë¸ |
| Scaler | `models/scaler.pkl` | í‘œì¤€í™” ìŠ¤ì¼€ì¼ëŸ¬ |
| Feature ëª©ë¡ | `models/feature_cols.json` | í•™ìŠµ í”¼ì²˜ ëª©ë¡ |
| ê²°ê³¼ JSON | `models/training_results.json` | ì „ì²´ ê²°ê³¼ |

---

> **ë‹¤ìŒ ë‹¨ê³„**: ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ë˜ëŠ” Risk Score ìƒì„±
"""
    
    # ì €ì¥
    report_path = report_dir / '01_ml_training_results.md'
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"\nğŸ“„ ë¦¬í¬íŠ¸ ì €ì¥: {report_path}")


# ============================================
# ë©”ì¸ íŒŒì´í”„ë¼ì¸
# ============================================
def run_training_pipeline(data_dir: Optional[Path] = None,
                          model_dir: Optional[Path] = None,
                          report_dir: Optional[Path] = None) -> Dict:
    """
    ì „ì²´ í•™ìŠµ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
    """
    if data_dir is None:
        data_dir = DATA_DIR
    if model_dir is None:
        model_dir = MODEL_DIR
    if report_dir is None:
        report_dir = REPORT_DIR
    
    print("=" * 60)
    print("ğŸš€ KKBox Churn Prediction - Model Training Pipeline")
    print("=" * 60)
    
    # 1. ë°ì´í„° ë¡œë“œ
    train, valid, test = load_datasets(data_dir)
    
    # 2. í”¼ì²˜ ì¤€ë¹„
    X_train, y_train, X_valid, y_valid, X_test, y_test, feature_cols = prepare_features(
        train, valid, test
    )
    
    # 3. Logistic Regression í•™ìŠµ
    lr_model, scaler, lr_results = train_logistic_regression(
        X_train, y_train, X_valid, y_valid
    )
    
    # 4. LightGBM í•™ìŠµ
    lgb_model, lgb_results = train_lightgbm(
        X_train, y_train, X_valid, y_valid
    )
    
    # 5. í…ŒìŠ¤íŠ¸ì…‹ í‰ê°€
    models = {
        'Logistic Regression': lr_model,
        'LightGBM': lgb_model
    }
    test_results = evaluate_on_test(models, X_test, y_test, scaler)
    
    # ê²°ê³¼ í†µí•©
    lr_results['test_metrics'] = test_results['Logistic Regression']
    lgb_results['test_metrics'] = test_results['LightGBM']
    
    all_results = {
        'Logistic Regression': lr_results,
        'LightGBM': lgb_results
    }
    
    # 6. ê²°ê³¼ ì €ì¥
    save_results(all_results, models, scaler, model_dir, feature_cols)
    
    # 7. ë¦¬í¬íŠ¸ ìƒì„±
    generate_report(all_results, report_dir)
    
    print("\n" + "=" * 60)
    print("âœ… í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
    print("=" * 60)
    
    return all_results


# ============================================
# ì‹¤í–‰
# ============================================
if __name__ == "__main__":
    results = run_training_pipeline()

