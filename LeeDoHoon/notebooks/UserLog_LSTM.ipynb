{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# User Log 기반 LSTM Churn Model\n",
        "\n",
        "> `user_logs_v2.csv` 원시 로그에서 30일 시퀀스를 만들고, LSTM으로 이탈 여부를 예측하는 노트북입니다.\n",
        "> \n",
        "> - 입력: (N, T, F) 시퀀스 (T=관측 일수, F=로그 피처 수)\n",
        "> - 타겟: `is_churn` (train_v2.csv 기준)\n",
        "> - 모델: PyTorch LSTM + MLP 헤드\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "RANDOM_STATE = 719\n",
        "np.random.seed(RANDOM_STATE)\n",
        "torch.manual_seed(RANDOM_STATE)\n",
        "\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "pd.set_option(\"display.max_rows\", 100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 라벨 및 로그 데이터 로드\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1) 라벨 로드 (is_churn)\n",
        "train = pd.read_csv(\"../data/train_v2.csv\")  # msno, is_churn 포함이라고 가정\n",
        "train = train[[\"msno\", \"is_churn\"]]\n",
        "\n",
        "print(\"Train shape:\", train.shape)\n",
        "train.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2) user_logs 로드\n",
        "logs = pd.read_csv(\"../data/user_logs_v2.csv\")\n",
        "print(\"Logs shape:\", logs.shape)\n",
        "logs.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 날짜 처리 및 관측 윈도우 필터링\n",
        "logs[\"date\"] = pd.to_datetime(logs[\"date\"])\n",
        "\n",
        "T = pd.Timestamp(\"2017-04-01\")\n",
        "start_date = pd.Timestamp(\"2017-03-01\")\n",
        "end_date   = pd.Timestamp(\"2017-03-31\")\n",
        "\n",
        "logs = logs[(logs[\"date\"] >= start_date) & (logs[\"date\"] <= end_date)]\n",
        "print(\"Filtered logs:\", logs.shape)\n",
        "logs.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. User별 (T, F) 시퀀스 텐서 생성\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 사용할 로그 피처\n",
        "seq_features = [\n",
        "    \"num_25\", \"num_50\", \"num_75\", \"num_985\", \"num_100\",\n",
        "    \"total_secs\",\n",
        "]\n",
        "\n",
        "# (msno, date) 기준 일별 합계\n",
        "daily = (\n",
        "    logs\n",
        "    .groupby([\"msno\", \"date\"], as_index=False)[seq_features]\n",
        "    .sum()\n",
        ")\n",
        "\n",
        "print(\"Daily shape:\", daily.shape)\n",
        "daily.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 전체 유저/날짜 인덱스 생성\n",
        "all_users = train[\"msno\"].unique()\n",
        "all_dates = pd.date_range(start_date, end_date, freq=\"D\")\n",
        "\n",
        "T_len = len(all_dates)\n",
        "F_dim = len(seq_features)\n",
        "print(\"T_len (window length):\", T_len)\n",
        "print(\"F_dim (feature dim):\", F_dim)\n",
        "\n",
        "# 빠른 lookup을 위한 인덱싱\n",
        "daily_indexed = daily.set_index([\"msno\", \"date\"])  # MultiIndex\n",
        "\n",
        "X_list = []\n",
        "y_list = []\n",
        "\n",
        "for msno, is_churn in train[[\"msno\", \"is_churn\"]].itertuples(index=False):\n",
        "    seq = np.zeros((T_len, F_dim), dtype=np.float32)\n",
        "    for t_idx, dt in enumerate(all_dates):\n",
        "        key = (msno, dt)\n",
        "        if key in daily_indexed.index:\n",
        "            row = daily_indexed.loc[key, seq_features]\n",
        "            seq[t_idx, :] = row.values.astype(np.float32)\n",
        "        # else: 활동 없음 → 0 유지\n",
        "    X_list.append(seq)\n",
        "    y_list.append(is_churn)\n",
        "\n",
        "X_seq = np.stack(X_list, axis=0)   # (N, T, F)\n",
        "y_arr = np.array(y_list, dtype=np.int64)\n",
        "\n",
        "print(\"X_seq shape:\", X_seq.shape)\n",
        "print(\"y_arr shape:\", y_arr.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. PyTorch Dataset / DataLoader 구성\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class UserLogDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)  # (N, T, F)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)  # (N,)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# train/valid/test split\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X_seq, y_arr,\n",
        "    test_size=0.3,\n",
        "    stratify=y_arr,\n",
        "    random_state=RANDOM_STATE,\n",
        ")\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(\n",
        "    X_temp, y_temp,\n",
        "    test_size=0.5,\n",
        "    stratify=y_temp,\n",
        "    random_state=RANDOM_STATE,\n",
        ")\n",
        "\n",
        "train_ds = UserLogDataset(X_train, y_train)\n",
        "valid_ds = UserLogDataset(X_valid, y_valid)\n",
        "test_ds  = UserLogDataset(X_test, y_test)\n",
        "\n",
        "train_dl = DataLoader(train_ds, batch_size=256, shuffle=True)\n",
        "valid_dl = DataLoader(valid_ds, batch_size=256)\n",
        "test_dl  = DataLoader(test_ds, batch_size=256)\n",
        "\n",
        "len(train_ds), len(valid_ds), len(test_ds)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. LSTM 모델 정의\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class UserLogLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=128, num_layers=2, bidirectional=True, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_dim,\n",
        "            hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=bidirectional,\n",
        "            dropout=dropout if num_layers > 1 else 0.0,\n",
        "        )\n",
        "        out_dim = hidden_dim * (2 if bidirectional else 1)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(out_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T, F)\n",
        "        out, _ = self.lstm(x)       # out: (B, T, H*D)\n",
        "        last_hidden = out[:, -1, :] # (B, H*D)\n",
        "        prob = self.head(last_hidden).squeeze(-1)  # (B,)\n",
        "        return prob\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = UserLogLSTM(input_dim=F_dim).to(device)\n",
        "model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 학습 루프 및 평가\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, recall_score, precision_score, f1_score\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# 클래스 불균형 보정용 pos_weight (원하면 BCEWithLogitsLoss + sigmoid 제거로 변경 가능)\n",
        "pos_weight = torch.tensor(\n",
        "    (y_train == 0).sum() / (y_train == 1).sum(),\n",
        "    dtype=torch.float32,\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "\n",
        "def train_one_epoch(model, dataloader):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for X_batch, y_batch in dataloader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(X_batch)           # (B,) 0~1\n",
        "        loss = F.binary_cross_entropy(y_pred, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * len(y_batch)\n",
        "    return total_loss / len(dataloader.dataset)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, dataloader):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    all_y = []\n",
        "    all_p = []\n",
        "    for X_batch, y_batch in dataloader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "\n",
        "        y_pred = model(X_batch)\n",
        "        loss = F.binary_cross_entropy(y_pred, y_batch)\n",
        "        total_loss += loss.item() * len(y_batch)\n",
        "\n",
        "        all_y.append(y_batch.cpu().numpy())\n",
        "        all_p.append(y_pred.cpu().numpy())\n",
        "\n",
        "    all_y = np.concatenate(all_y)\n",
        "    all_p = np.concatenate(all_p)\n",
        "    return total_loss / len(dataloader.dataset), all_y, all_p\n",
        "\n",
        "\n",
        "def compute_metrics(y_true, y_proba, thr=0.5):\n",
        "    y_pred = (y_proba >= thr).astype(int)\n",
        "    return {\n",
        "        \"roc_auc\": roc_auc_score(y_true, y_proba),\n",
        "        \"pr_auc\": average_precision_score(y_true, y_proba),\n",
        "        \"recall\": recall_score(y_true, y_pred),\n",
        "        \"precision\": precision_score(y_true, y_pred),\n",
        "        \"f1\": f1_score(y_true, y_pred),\n",
        "    }\n",
        "\n",
        "\n",
        "n_epochs = 10\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    train_loss = train_one_epoch(model, train_dl)\n",
        "    valid_loss, y_val, p_val = evaluate(model, valid_dl)\n",
        "    metrics = compute_metrics(y_val, p_val, thr=0.5)\n",
        "    print(\n",
        "        f\"Epoch {epoch:02d} | \"\n",
        "        f\"train_loss={train_loss:.4f} | valid_loss={valid_loss:.4f} | \"\n",
        "        f\"ROC-AUC={metrics['roc_auc']:.4f} | PR-AUC={metrics['pr_auc']:.4f} | Recall={metrics['recall']:.4f}\"\n",
        "    )\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
